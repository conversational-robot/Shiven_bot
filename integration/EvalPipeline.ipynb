{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2GstRR_BzCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc0ed52f-a1a8-471f-d6a6-452d1bc72971",
        "tags": []
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import shutil\n",
        "import itertools\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arzAwtLQB0r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = s.lower().strip()\n",
        "    \n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    \n",
        "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "\n",
        "    s = s.rstrip().strip()\n",
        "   \n",
        "    s = '<start> ' + s + ' <end>'\n",
        "    return s\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs8HNNBvB5cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "embedding_dims = 200\n",
        "rnn_units = 512\n",
        "dense_units = 512\n",
        "Dtype = tf.float32 \n",
        "Tx=Ty=24  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SrRiklIDT1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('X_tokenizer.pickle', 'rb') as handle:\n",
        "    X_tokenizer=Y_tokenizer=pickle.load(handle)\n",
        "input_vocab_size = len(X_tokenizer.word_index)+1  \n",
        "output_vocab_size = len(Y_tokenizer.word_index)+ 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giyKkzGrCBRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 200\n",
        "num_words= input_vocab_size\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim)) \n",
        "with open('embedding_matrix.pickle', 'rb') as handle:\n",
        "    embedding_matrix=pickle.load(handle)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-ksEXYXEqlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        # self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "        #                                                    output_dim=embedding_dims)\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(num_words, embedding_dim, input_length=Tx,weights=[embedding_matrix],trainable=False)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "        #self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     #return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        # self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "        #                                                    output_dim=embedding_dim) \n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(num_words, embedding_dim, input_length=Tx,weights=[embedding_matrix],trainable=False)\n",
        "\n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZkxziuvE-PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "\n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4H3UW3WGdOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "status=checkpoint.restore(tf.train.latest_checkpoint('training_checkpoints')).expect_partial()\n",
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    'training_checkpoints', 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfDHs3aTGsRG",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "def responder(input_raw):\n",
        "    beam_width = 3\n",
        "    input_lines = [preprocess_sentence(input_raw)]\n",
        "    input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "    input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "    inp = tf.convert_to_tensor(input_sequences)\n",
        "\n",
        "    inference_batch_size = 1\n",
        "    encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "\n",
        "    end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "    decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "    encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "\n",
        "    decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "    encoder_state = tfa.seq2seq.tile_batch([a_tx, c_tx], multiplier=beam_width)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "\n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "    beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "    for j in range(maximum_iterations):\n",
        "        beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "        scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "        beam_scores = np.append(beam_scores, scores, axis = -1)                                                                         \n",
        "    print(input_raw)\n",
        "    print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[0,:,:]\n",
        "    score_beams_per_sample = beam_scores[0,:,:]\n",
        "    best_response=\"\"\n",
        "    least_score=1000\n",
        "    for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=2, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        response = \" \".join( [Y_tokenizer.index_word[w] for w in seq])\n",
        "        print(response, \" beam score: \", beam_score)\n",
        "        if beam_score<least_score:\n",
        "            least_score=beam_score\n",
        "            best_response= response\n",
        "    return best_response    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr  \n",
        "\n",
        "def speechInput(): \n",
        "    r = sr.Recognizer()  \n",
        "    with sr.Microphone() as source:  \n",
        "        print(\"Please wait. Calibrating microphone...\")  \n",
        "        r.adjust_for_ambient_noise(source, duration=1)  \n",
        "        print(\"Say something!\")  \n",
        "        audio = r.listen(source)   \n",
        "    try: \n",
        "        speech= r.recognize_google(audio)\n",
        "        print(\"You said: '\" + speech + \"'\")\n",
        "        return speech  \n",
        "    except sr.UnknownValueError:  \n",
        "        print(\"I could not understand audio :(\")  \n",
        "    except sr.RequestError as e:  \n",
        "        print(\"Recog error; {0}\".format(e))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "from time import sleep\n",
        "import os\n",
        "import pyglet\n",
        "\n",
        "def speakResponse(response):\n",
        "    tts = gTTS(text=response, lang='en')\n",
        "    filename = '/tmp/temp.mp3'\n",
        "    tts.save(filename)\n",
        "\n",
        "    music = pyglet.media.load(filename, streaming=False)\n",
        "    music.play()\n",
        "\n",
        "    sleep(music.duration) \n",
        "    os.remove(filename) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while input(\"Continue?(y/n)\") is 'y':\n",
        "    print(responder(str(input())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Please wait. Calibrating microphone...\nSay something!\nYou said: 'hello'\nhello\n---------------------------------------------\nyou re , old man  beam score:  -20.961727\ni are a good . .  beam score:  -29.208097\ncome on years phone ,  beam score:  -24.903067\n"
        }
      ],
      "source": [
        "while input(\"Continue?(y/n)\") is 'y':\n",
        "    speech=speechInput()\n",
        "    response=responder(speech)\n",
        "    speakResponse(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "EvalPipeline",
      "provenance": []
    },
    "kernelspec": {
      "name": "python36764bitconvconda8f406cab152b4c939490edccd8ec77fd",
      "display_name": "Python 3.6.7 64-bit ('conv': conda)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}